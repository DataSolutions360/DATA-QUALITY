## 1) Descriptive Statistics:

- Summary Statistics: Calculating measures such as mean, median, mode, standard deviation, and range to understand the central tendency and dispersion of data.
- Frequency Distribution: Creating histograms or frequency tables to visualize the distribution of data values.

  - NUMERICAL DATA:
    - T-test = Comparing means of 2 groups, when population SD is NOT KNOWN (n<30)...better for smaller samples
        - When to use t-test:
          -  Population S.D. is UNKNOWN
          -  Small Sample Size
          -  Testing for Sample Mean(you test a hypothesis related to the sample mean, such as monpasing means between 2 or more groups or testing the diff bt  a sample mean and hypothesized pop mean)
          -  Data Distribution Assumption(assuming the data follows approximate NORMAL DISTRIBUTION or that the Central Limit Theorem holds = as sample size increases, the data normalizes)   
    - Z-score = Comparing means of 2 groups, when population SD is NOT KNOWN (n<30)...better for smaller samples
        -  When to use Z-score:
          -  Population S.D is KNOWN
          -  Large Sample Size
          -  Data Follows a Normal Distribution
    - ANOVA = comparing multiple means
    - Correlation Analysis
    - Regression Analysis

  - CATEGORICAL DATA:
    - Chi-Square test = used to determine if there is a significant association between 2 categorical variables.
        - Goodness of Fit test = When you want to compare the observed distribution of a single categorical variable to an expected distribution.  i.e. testing if the observed distribution of blood types in a population matches the expected distribution based on genetics.
    - Contingency Tables =
    -  Logistical Regression = 

## 2) Data Profiling:

- Data Profiling: Examining the basic characteristics of data, including data types, unique values, missing values, and data patterns.

## 3) Data Cleansing:

- Outlier Detection: Identifying and handling outliers using techniques like the Z-score or the IQR (Interquartile Range) method.
- Missing Data Imputation: Filling in missing data points using methods such as mean imputation, median imputation, or predictive modeling.

## 4) Data Validation:

- Cross-Validation: Checking data integrity by verifying relationships between related fields or tables.
- Referential Integrity: Ensuring that foreign keys in relational databases reference valid primary keys.

## 5) Data Quality Metrics:

- Data Quality Score: Calculating a composite score based on multiple data quality dimensions (e.g., completeness, accuracy, consistency) to assess overall data quality.
- Error Rate: Quantifying the proportion of data records or entries with errors.

## 6) Data Profiling Tools:

- Using data profiling tools and software that automate the process of analyzing data to identify anomalies, patterns, and issues.

## 7) Distribution Analysis:

- Assessing whether data follows specific probability distributions (e.g., normal, binomial) through goodness-of-fit tests like the Kolmogorov-Smirnov test or Chi-Square test.

## 8) Correlation and Regression:

- Studying relationships between variables using correlation analysis (e.g., Pearson's correlation coefficient) or regression analysis (e.g., linear regression).

## 9) Sampling Techniques:

- Conducting random sampling or stratified sampling to assess data quality on a subset of data rather than the entire dataset.

## 10) Data Quality Dashboards:

- Creating interactive dashboards with visualizations to monitor data quality metrics and trends over time.

## 11) Data Profiling with Machine Learning:

- Leveraging machine learning algorithms for data profiling tasks, anomaly detection, and predictive data quality assessment.

## 12) Record Linkage and Deduplication:

- Identifying and merging duplicate records within a dataset, which is important for maintaining data consistency.

## 13) Time Series Analysis:

- Analyzing data collected over time to detect trends, seasonality, and irregularities.

## 14) Geospatial Analysis:

- Assessing data quality in geospatial datasets by checking for spatial accuracy, consistency, and completeness.

## 15) Text Analysis:

- Applying natural language processing (NLP) techniques to assess text data quality, including sentiment analysis and text classification.
